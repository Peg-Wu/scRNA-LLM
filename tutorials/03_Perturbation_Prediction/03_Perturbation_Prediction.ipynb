{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "from gears import PertData\n",
    "from transformers import set_seed\n",
    "from typing import List, Dict, Optional\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "from gears.inference import deeper_analysis, non_dropout_analysis\n",
    "from stella.models.modeling_stella import STELLAForPerturbation\n",
    "from stella.utils import map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367581f",
   "metadata": {},
   "source": [
    "### 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa08278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data prcocessing\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536 # 1536\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4\n",
    "batch_size = 12 # 12\n",
    "eval_batch_size = 12 # 12\n",
    "epochs = 10\n",
    "schedule_interval = 1\n",
    "early_stop = 3\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# logging steps for training\n",
    "log_interval = 200\n",
    "\n",
    "# Here we use the same dataset as GEARS, you can also use your own dataset.\n",
    "# !!! If you want to use your own dataset, please process your dataset first !!!\n",
    "data_name = \"norman\"  # dixit, norman, adamson\n",
    "split = \"simulation\"\n",
    "\n",
    "# visualization\n",
    "if data_name == \"norman\":\n",
    "    perts_to_plot = [\"SAMD1+ZBTB1\"]\n",
    "elif data_name == \"adamson\":\n",
    "    perts_to_plot = [\"KCTD16+ctrl\"]\n",
    "\n",
    "# save\n",
    "save_dir = Path(f\"./save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(f\"saving to {save_dir}\")\n",
    "logging.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdfc48",
   "metadata": {},
   "source": [
    "### 2. Load Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3eab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_data = PertData(\"/data/home/tuser/liyazi/STELLA_Beta-main/tutorials/03_Perturbation_Prediction/data\")\n",
    "pert_data.load(data_name=data_name)\n",
    "pert_data.prepare_split(split=split, seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08127b",
   "metadata": {},
   "source": [
    "### 3. Convert gene symbol to id according to STELLA vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../src/stella/gene2id.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "ntokens = len(vocab)\n",
    "print(ntokens)\n",
    "\n",
    "# View num_genes in STELLA vocab\n",
    "num_genes_in_vocab = pert_data.adata.var[\"gene_name\"].isin(vocab.keys()).sum()\n",
    "logging.info(f\"Number of genes in stella vocab: {num_genes_in_vocab} / {len(pert_data.adata.var['gene_name'])}\")\n",
    "\n",
    "genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "n_genes = len(genes)\n",
    "\n",
    "# If gene symbol is not in vocab, use [PAD] token id to represent it.\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"[PAD]\"] for gene in genes], dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff94dc",
   "metadata": {},
   "source": [
    "### 4. Load Pretrained Model and Freeze Some Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STELLAForPerturbation.from_pretrained(\n",
    "    \"../../pretrained_models/B100_L2048\", input_gene_expr_type=\"continuous\"\n",
    ")\n",
    "\n",
    "def freeze_first_k_layers(k=4):\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(f\"stella.encoder.layer.{i}\" in name for i in range(k)):\n",
    "            param.requires_grad = False\n",
    "\n",
    "# freeze the first k layers\n",
    "freeze_first_k_layers(k=0)  # no freeze\n",
    "\n",
    "# check the trainable status of the parameters\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, \"\\t\", params.requires_grad)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f3c02",
   "metadata": {},
   "source": [
    "### 5. Define the functions needed for training, validation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_loader(batch_data, split_input_gene: bool, max_seq_len: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Process batch_data to get the input and target tensors.\n",
    "    \"\"\"\n",
    "    batch_data = batch_data.to(device)\n",
    "    batch_size = len(batch_data.pert)  # batch size\n",
    "    \n",
    "    ori_gene_values = batch_data.x[:, 0].view(batch_size, n_genes)\n",
    "    pert_flags = batch_data.x[:, 1].view(batch_size, n_genes)\n",
    "    \n",
    "    if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "        if include_zero_gene == \"all\":\n",
    "            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            input_gene_ids = ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "        if split_input_gene:\n",
    "            if len(input_gene_ids) > max_seq_len:\n",
    "                # method 1: perturb gene should be in max_seq_len\n",
    "                exclude = pert_flags.nonzero()[:, -1].unique()\n",
    "                available = input_gene_ids[~torch.isin(input_gene_ids, exclude)]\n",
    "                input_gene_ids = available[torch.randperm(len(available), device=device)[:max_seq_len-len(exclude)]]\n",
    "                input_gene_ids = torch.cat([exclude, input_gene_ids])\n",
    "\n",
    "                # method 2: perturb gene can not be in max_seq_len\n",
    "                # input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[:max_seq_len]\n",
    "\n",
    "        mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "        mapped_input_gene_ids = mapped_input_gene_ids.repeat(len(batch_data.pert), 1).to(dtype=torch.int32)\n",
    "        input_values = ori_gene_values[:, input_gene_ids].to(dtype=torch.float32)   # [batch size, seq_len]\n",
    "        input_pert_flags = pert_flags[:, input_gene_ids].to(dtype=torch.int32)   # [batch size, seq_len]\n",
    "        \n",
    "        if batch_data.y is not None:\n",
    "            target_gene_values = batch_data.y.view(batch_size, n_genes)  # [batch size, seq_len]\n",
    "            target_values = target_gene_values[:, input_gene_ids].to(dtype=torch.float32)  # [batch size, seq_len]\n",
    "        else:\n",
    "            target_values = None\n",
    "\n",
    "    return mapped_input_gene_ids, input_values, input_pert_flags, target_values\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: torch.utils.data.DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    for batch, batch_data in enumerate(train_loader):\n",
    "        with autocast():\n",
    "            mapped_input_gene_ids, input_values, input_pert_flags, target_values = process_loader(\n",
    "                batch_data, \n",
    "                split_input_gene=True, \n",
    "                max_seq_len=max_seq_len\n",
    "                )\n",
    "\n",
    "            output_dict = model(\n",
    "                input_ids_gene_symbol = mapped_input_gene_ids,\n",
    "                input_ids_gene_expression = input_values,\n",
    "                input_pert_flags = input_pert_flags,\n",
    "                labels = target_values,\n",
    "                output_attentions = False,\n",
    "                output_hidden_states = False,\n",
    "            )\n",
    "\n",
    "            loss = output_dict['loss']\n",
    "            \n",
    "            # amp training\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # non-amp training\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = total_loss / log_interval\n",
    "                logging.info(\n",
    "                    f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                    f\"lr {lr:05.5f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                    f\"loss {cur_loss:5.4f} | grad_norm {grad_norm:5.4f} | \"\n",
    "                )\n",
    "                total_loss = 0.0\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "def eval_perturb(model: STELLAForPerturbation, loader: DataLoader) -> Dict:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "\n",
    "    for itr, batch_data in enumerate(loader):\n",
    "        pert_cat.extend(batch_data.pert)\n",
    "        mapped_input_gene_ids, input_values, input_pert_flags, target_values = process_loader(\n",
    "            batch_data, \n",
    "            split_input_gene=False\n",
    "            )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_dict = model(\n",
    "            input_ids_gene_symbol = mapped_input_gene_ids,\n",
    "            input_ids_gene_expression = input_values,\n",
    "            input_pert_flags = input_pert_flags,\n",
    "            labels = target_values,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            )\n",
    "            p = output_dict[\"logits\"]\n",
    "            t = batch_data.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "\n",
    "            for itr, de_idx in enumerate(batch_data.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "\n",
    "    results[\"pert_cat\"] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results[\"pred\"] = pred.detach().cpu().numpy().astype(np.float32)\n",
    "    results[\"truth\"] = truth.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results[\"pred_de\"] = pred_de.detach().cpu().numpy().astype(np.float32)\n",
    "    results[\"truth_de\"] = truth_de.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: STELLAForPerturbation, pert_list: List[str], query, pool_size: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Predict the gene expression values for the given perturbations.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`torch.nn.Module`): The model to use for prediction.\n",
    "        pert_list (:obj:`List[str]`): The list of perturbations to predict.\n",
    "        pool_size (:obj:`int`, optional): For each perturbation, use this number\n",
    "            of cells in the control and predict their perturbation results. Report\n",
    "            the stats of these predictions. If `None`, use all control cells.\n",
    "    \"\"\"\n",
    "    adata = pert_data.adata\n",
    "    ctrl_adata = adata[adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    if pool_size is None:\n",
    "        pool_size = len(ctrl_adata.obs)\n",
    "    gene_list = pert_data.gene_names.values.tolist()\n",
    "    for pert in pert_list:\n",
    "        for i in pert:\n",
    "            if i not in gene_list:\n",
    "                raise ValueError(\n",
    "                    \"The gene is not in the perturbation graph. Please select from GEARS.gene_list!\"\n",
    "                )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        results_pred = {}\n",
    "        for pert in pert_list:\n",
    "            cell_graphs = create_cell_graph_dataset_for_prediction(\n",
    "                pert, ctrl_adata, gene_list, device, num_samples=pool_size\n",
    "            )\n",
    "            loader = DataLoader(cell_graphs, batch_size=eval_batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch_data in loader:\n",
    "                mapped_input_gene_ids, input_values, input_pert_flags, target_values = process_loader(\n",
    "                batch_data, \n",
    "                split_input_gene=False\n",
    "                )\n",
    "                output_dict = model(\n",
    "                input_ids_gene_symbol = mapped_input_gene_ids,\n",
    "                input_ids_gene_expression = input_values,\n",
    "                input_pert_flags = input_pert_flags,\n",
    "                labels = target_values,\n",
    "                output_attentions = False,\n",
    "                output_hidden_states = False,\n",
    "                )\n",
    "                pred_gene_values = output_dict[\"logits\"]\n",
    "                preds.append(pred_gene_values)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            results_pred[\"_\".join(pert)] = np.mean(preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return results_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e5f50",
   "metadata": {},
   "source": [
    "### 6. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_corr = 0\n",
    "best_model = None\n",
    "patience = 0\n",
    "train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "valid_loader = pert_data.dataloader[\"val_loader\"]\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_loader)\n",
    "    val_res = eval_perturb(model, valid_loader)\n",
    "    val_metrics = compute_perturbation_metrics(\n",
    "        val_res, pert_data.adata[pert_data.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    )\n",
    "    logging.info(f\"val_metrics at epoch {epoch}: \")\n",
    "    logging.info(val_metrics)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logging.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "    val_score = val_metrics[\"pearson_delta\"]\n",
    "    if val_score > best_val_corr:\n",
    "        best_val_corr = val_score\n",
    "        best_model = copy.deepcopy(model)\n",
    "        logging.info(f\"Best model with score (pearson_delta) {val_score:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logging.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68085115",
   "metadata": {},
   "source": [
    "### 7. Plot Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perturbation(\n",
    "    model: nn.Module, query: str, save_file: str = None, pool_size: int = None\n",
    ") -> matplotlib.figure.Figure:\n",
    "    sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "\n",
    "    adata = pert_data.adata\n",
    "    gene2idx = pert_data.node_map\n",
    "    cond2name = dict(adata.obs[[\"condition\", \"condition_name\"]].values)\n",
    "    gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
    "\n",
    "    de_idx = [\n",
    "        gene2idx[gene_raw2id[i]]\n",
    "        for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    genes = [\n",
    "        gene_raw2id[i] for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
    "    if query.split(\"+\")[1] == \"ctrl\":\n",
    "        pert_list = [[query.split(\"+\")[0]]]\n",
    "        pred = predict(model, pert_list, query, pool_size=pool_size)\n",
    "        pred = pred[query.split(\"+\")[0]][de_idx]\n",
    "    else:\n",
    "        pert_list = [query.split(\"+\")]\n",
    "        pred = predict(model, pert_list, query, pool_size=pool_size)\n",
    "        pred = pred[\"_\".join(query.split(\"+\"))][de_idx]\n",
    "    ctrl_means = adata[adata.obs[\"condition\"] == \"ctrl\"].to_df().mean()[de_idx].values\n",
    "\n",
    "    pred = pred - ctrl_means\n",
    "    truth = truth - ctrl_means\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[16.5, 4.5])\n",
    "    plt.title(query)\n",
    "    plt.boxplot(truth, showfliers=False, medianprops=dict(linewidth=0))\n",
    "\n",
    "    for i in range(pred.shape[0]):\n",
    "        _ = plt.scatter(i + 1, pred[i], color=\"red\")\n",
    "\n",
    "    plt.axhline(0, linestyle=\"dashed\", color=\"green\")\n",
    "\n",
    "    ax.xaxis.set_ticklabels(genes, rotation=90)\n",
    "\n",
    "    plt.ylabel(\"Change in Gene Expression over Control\", labelpad=10)\n",
    "    plt.tick_params(axis=\"x\", which=\"major\", pad=5)\n",
    "    plt.tick_params(axis=\"y\", which=\"major\", pad=5)\n",
    "    sns.despine()\n",
    "\n",
    "    if save_file:\n",
    "        fig.savefig(save_file, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if best_model is None:\n",
    "    # load best model from saved file\n",
    "    best_model = STELLAForPerturbation.from_pretrained(\"../../pretrained_models/B100_L2048\", input_gene_expr_type = \"continuous\").to(device)\n",
    "    save_dir = Path('./save/dev_perturb_norman-May11-19-54')\n",
    "    model_path = save_dir / \"best_model.pt\"\n",
    "    best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "for p in perts_to_plot:\n",
    "    plot_perturbation(best_model, p, pool_size=300, save_file=f\"{save_dir}/{p}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f05239",
   "metadata": {},
   "source": [
    "### 8. Test results and deeper analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a738e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = pert_data.dataloader[\"test_loader\"]\n",
    "test_res = eval_perturb(best_model, test_loader)\n",
    "test_metrics = compute_perturbation_metrics(\n",
    "    test_res, pert_data.adata[pert_data.adata.obs[\"condition\"] == \"ctrl\"]\n",
    ")\n",
    "print(test_metrics)\n",
    "test_metrics = {k: float(v) if isinstance(v, np.float32) else v for k, v in test_metrics.items()}\n",
    "# save test results\n",
    "with open(f\"{save_dir}/test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f)\n",
    "\n",
    "deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "\n",
    "metrics = [\"pearson_delta\", \"pearson_delta_de\"]\n",
    "metrics_non_dropout = [\n",
    "    \"pearson_delta_top20_de_non_dropout\",\n",
    "    \"pearson_top20_de_non_dropout\",\n",
    "]\n",
    "subgroup_analysis = {}\n",
    "for name in pert_data.subgroup[\"test_subgroup\"].keys():\n",
    "    subgroup_analysis[name] = {}\n",
    "    for m in metrics:\n",
    "        subgroup_analysis[name][m] = []\n",
    "\n",
    "    for m in metrics_non_dropout:\n",
    "        subgroup_analysis[name][m] = []\n",
    "\n",
    "for name, pert_list in pert_data.subgroup[\"test_subgroup\"].items():\n",
    "    for pert in pert_list:\n",
    "        for m in metrics:\n",
    "            subgroup_analysis[name][m].append(deeper_res[pert][m])\n",
    "\n",
    "        for m in metrics_non_dropout:\n",
    "            subgroup_analysis[name][m].append(non_dropout_res[pert][m])\n",
    "\n",
    "for name, result in subgroup_analysis.items():\n",
    "    for m in result.keys():\n",
    "        mean_value = np.mean(subgroup_analysis[name][m])\n",
    "        logging.info(\"test_\" + name + \"_\" + m + \": \" + str(mean_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
